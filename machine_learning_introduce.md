## æœºå™¨å­¦ä¹ ä»‹ç»

#### <font color='blue' face='æ¥·ä½“'>æœºå™¨å­¦ä¹ </font>

â€‹	æœºå™¨å­¦ä¹ æ˜¯ä¸€é—¨å¤šå­¦ç§‘äº¤å‰ä¸“ä¸šï¼Œæ¶µç›–æ¦‚ç‡è®ºçŸ¥è¯†ï¼Œç»Ÿè®¡å­¦çŸ¥è¯†ï¼Œè¿‘ä¼¼ç†è®ºçŸ¥è¯†å’Œå¤æ‚ç®—æ³•çŸ¥è¯†ï¼Œä½¿ç”¨è®¡ç®—æœºä½œä¸ºå·¥å…·å¹¶è‡´åŠ›äºçœŸå®å®æ—¶çš„æ¨¡æ‹Ÿäººç±»å­¦ä¹ æ–¹å¼ï¼Œå¹¶å°†ç°æœ‰å†…å®¹è¿›è¡ŒçŸ¥è¯†ç»“æ„åˆ’åˆ†æ¥æœ‰æ•ˆæé«˜å­¦ä¹ æ•ˆç‡ã€‚

â€‹	å¾ˆéš¾æ˜ç¡®çš„å®šä¹‰ï¼Œç®€å•çš„è¯´ï¼Œæœºå™¨å­¦ä¹ å°±æ˜¯åˆ©ç”¨æ•°å­¦æ–¹æ³•å’Œè®¡ç®—æœºæŠ€æœ¯é€šè¿‡å¯¹å†å²æ•°æ®è¿›è¡Œåˆ†æå¾—åˆ°è§„å¾‹ï¼ˆæ¨¡å‹ï¼‰ï¼Œå¹¶åˆ©ç”¨è§„å¾‹å¯¹æœªçŸ¥æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚

#### <font face='æ¥·ä½“' color='blue'>æ•°æ®é›†</font>

â€‹    æœºå™¨å­¦ä¹ æ˜¯ä»å†å²æ•°æ®è·å¾—è§„å¾‹ï¼Œé‚£è¿™äº›å†å²æ•°æ®æ˜¯æ€æ ·çš„?

#### <font face='æ¥·ä½“' color='blue'>å¯ä»¥è·å–çš„æ•°æ®é›†</font>

â€‹	1.scikit-learnæ•°æ®é‡è¾ƒå°ï¼Œæ–¹ä¾¿å­¦ä¹ 

â€‹	2.kaggleå¤§æ•°æ®ç«èµ›å¹³å°ï¼ŒçœŸå®æ•°æ®ï¼Œæ•°æ®é‡å·¨å¤§ã€‚å›½å†…ç±»ä¼¼çš„åƒé˜¿é‡Œçš„å¤©æ± ï¼Œç™¾åº¦çš„AI Studio

â€‹	3.UCIæ”¶å½•äº†360ä¸ªæ•°æ®é›†ï¼Œè¦†ç›–ç”Ÿæ´»ã€ç§‘å­¦ã€ç»æµç­‰é¢†åŸŸï¼Œæ•°æ®é‡å‡ åä¸‡

#### <font color='blue' face='æ¥·ä½“'>å¸¸è§çš„æ•°æ®é›†ç»“æ„ç»„æˆï¼š</font><font color='red' face='æ¥·ä½“'>ç‰¹å¾å€¼+ç›®æ ‡å€¼</font>



![dataset](./images/dataset.png)



  <font color='red'>æ³¨æ„ </font>: æœ‰äº›æ•°æ®é›†å¯ä»¥æ²¡æœ‰ç›®æ ‡å€¼ ã€‚æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªæ ·æœ¬ã€‚ æ¯ä¸€åˆ—å°±æ˜¯ä¸€ä¸ªç‰¹å¾ã€‚ æœ€åè¦`é¢„æµ‹çš„å€¼`å°±æ˜¯ç›®æ ‡ã€‚



#### <font color='blue' face='æ¥·ä½“'>äººå·¥æ™ºèƒ½</font>

â€‹	å¤§æ•°æ®äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œåœ¨åº”ç”¨å±‚é¢åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€ç¥ç»ç½‘ç»œã€æ·±åº¦å­¦ä¹ ç­‰ï¼Œå®ƒä»¬éƒ½æ˜¯ç°ä»£äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚åœ¨å¤§æ•°æ®èƒŒæ™¯ä¸‹ï¼Œè¿™äº›æŠ€æœ¯å‡å¾—åˆ°äº†è´¨çš„æå‡ï¼Œäººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒ…å«å…³ç³»å¦‚ä¸‹å›¾ã€‚

![åŒ…å«å…³ç³»](./images/åŒ…å«å…³ç³».png)


$
äººå·¥æ™ºèƒ½ = ç®—æ³• + ç®—åŠ› + ç®—æ–™
$


â€‹	`ç®—æ³•`æ˜¯ä¾é äººè®¾è®¡çš„ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡<font color='red'>æ·±åº¦å­¦ä¹ </font>ä¸æ–­çš„æ ¡æ­£ã€‚å› æ­¤æœ¬è´¨ä¸Šæ¥è¯´ï¼Œç®—æ³•æ˜¯äººçš„æ™ºåŠ›æ°´å¹³çš„ä½“ç°ã€‚æ—¢ç„¶æ˜¯äººçš„æ™ºåŠ›æ°´å¹³ä½“ç°ï¼Œå°±çœ‹äººæ‰çš„è´¨é‡äº†ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆç¡…è°·å‡ºç°å¤§é‡AIäººæ‰çš„åŸå› ï¼Œå› ä¸ºèšé›†äº†é¡¶çº§çš„æ™ºåŠ›èµ„æºã€‚

â€‹	`ç®—åŠ›`æ˜¯æœºå™¨è¿ç®—çš„å¿«æ…¢ï¼Œå› æ­¤å°±æ˜¯ç¡¬ä»¶è€Œå·²ï¼ŒèŠ±é’±å †å å°±è¡Œäº†ã€‚ç®—åŠ›æ–¹é¢GPUå…·æœ‰ç»å¯¹çš„ä¼˜åŠ¿ï¼Œç›®å‰å¤§éƒ¨åˆ†æ·±åº¦å­¦ä¹ çš„æ¡†æ¶ï¼Œæ¯”å¦‚`Tensorflow`ï¼Œ`Pytorch`éƒ½æ˜¯ä¾èµ–GPUæ¥å®Œæˆè®¡ç®—ä»»åŠ¡ã€‚

â€‹	`ç®—æ–™`æ˜¯ç»™AIå–‚å¤§é‡æ•°æ®ï¼Œå› æ­¤è°æ‰‹é‡Œæœ‰å¤§é‡çš„æ•°æ®ï¼Œè°çš„AIæ¨¡å‹å°±æ›´å‡†ç¡®ï¼ŒåŒæ—¶é€šè¿‡ä¸æ–­çš„<font color='red'>æ·±åº¦å­¦ä¹ </font>åå‘æ ¡æ­£ç®—æ³•ã€‚

#### <font color='blue' face='æ¥·ä½“'>scikit-learn</font>

â€‹	**scikit-learnæ˜¯åŸºäºPythonè¯­è¨€çš„æœºå™¨å­¦ä¹ å·¥å…·**

â€‹	1.ç®€å•é«˜æ•ˆçš„æ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†æå·¥å…·

â€‹	2.å¯ä¾›å¤§å®¶åœ¨å„ç§ç¯å¢ƒä¸­é‡å¤ä½¿ç”¨

â€‹	3.å»ºç«‹åœ¨Numpyï¼ŒScipyå’ŒMatplotlibä¸Š

â€‹	4.å¼€æºï¼Œå¯å•†ä¸šä½¿ç”¨-BSDè®¸å¯è¯

#### <font color='blue' face='æ¥·ä½“'>Scikit-learnæ•°æ®é›†APIä»‹ç»</font>

```python
1. sklearn.datasets
    1.1 åŠ è½½è·å–æµè¡Œæ•°æ®é›†
    1.2 datasets.load_*() -- è·å–å°è§„æ¨¡æ•°æ®é›†ï¼Œæ•°æ®åŒ…å«åœ¨datasetsé‡Œ
    1.3 datasetss.fetch_*(data_home=None)
    è·å–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦ä»ç½‘ç»œä¸Šä¸‹è½½ï¼Œå‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯data_homeï¼Œ è¡¨ç¤ºæ•°æ®é›†ã€‚ä¸‹è½½ç›®å½•,é»˜è®¤æ˜¯-/scikit-learn_data/
2. load_* å’Œ fetch_* è¿”å›çš„æ•°æ®ç±»å‹æ˜¯datasets.base.Bunch(å­—å…¸æ ¼å¼)
    data:ç‰¹å¾æ•°æ®æ•°ç»„,æ˜¯[n_ samples*n_features]çš„äºŒç»´numpy.ndarrayæ•°ç»„
    target:æ ‡ç­¾æ•°ç»„,æ˜¯n_samplesçš„ç»´numpy.ndarrayæ•°ç»„
    DESCR:æ•°æ®æè¿°
    feature_names:ç‰¹å¾åï¼Œæ–°é—»æ•°æ®ï¼Œæ‰‹å†™æ•°å­—ï¼Œå›å½’æ•°æ®é›†æ²¡æœ‰
    target_names;æ ‡ç­¾å
    #å…³äºç¬¬äºŒç‚¹, load_* ç”¨äºè·å–å°æ•°æ®é›† , fetch_* ç”¨äºè·å–å¤§æ•°æ®é›†
```



**scikit-learnçš„ä½¿ç”¨**

```python
# å¯¼å…¥æ–¹å¼
from sklearn.datasets import load_iris    # load_iris  å¯¼å…¥æ˜¯é¸¢å°¾èŠ±çš„æ•°æ®
 
# åŠ è½½é¸¢å°¾èŠ±çš„æ•°æ®
li = load_iris()

print('è·å–ç‰¹å¾å€¼',li.data)  # é¸¢å°¾èŠ±çš„ç‰¹å¾ï¼Œå®˜æ–¹æ—©å·²åˆ†ç±»å¥½çš„ï¼Œå¯ä¾›ç›´æ¥ä½¿ç”¨
print('ç›®æ ‡å€¼',li.target)    # åˆ†äº†3ä¸ªç±»

li.DESCR  # é¸¢å°¾èŠ±çš„æè¿°li.feature_names  # é¸¢å°¾èŠ±çš„ç‰¹å¾å  èŠ±é•¿  èŠ±å®½li.target_names  # é¸¢å°¾èŠ±çš„æ ‡ç­¾å
```



```
#1 ç‰¹å¾å€¼  # å€¼å¤ªå¤šï¼Œåªå¤åˆ¶ä¸€éƒ¨åˆ†å±•ç¤º
è·å–ç‰¹å¾å€¼
 [[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]
 [5.4 3.9 1.7 0.4]
 [4.6 3.4 1.4 0.3]
 [5.  3.4 1.5 0.2]
 [4.4 2.9 1.4 0.2]
 [4.9 3.1 1.5 0.1]
 [5.4 3.7 1.5 0.2]
 [4.8 3.4 1.6 0.2]
 [4.8 3.  1.4 0.1]
 [4.3 3.  1.1 0.1]
 [5.8 4.  1.2 0.2]
 [5.7 4.4 1.5 0.4]
 [5.4 3.9 1.3 0.4]
 [5.1 3.5 1.4 0.3]
 [5.7 3.8 1.7 0.3]
 [5.1 3.8 1.5 0.3]
 [5.4 3.4 1.7 0.2]
 [5.1 3.7 1.5 0.4]
 [4.6 3.6 1.  0.2]
 [5.1 3.3 1.7 0.5]
 [4.8 3.4 1.9 0.2]
 [5.  3.  1.6 0.2]
 [5.  3.4 1.6 0.4]
 [5.2 3.5 1.5 0.2]
 [5.2 3.4 1.4 0.2]
 [4.7 3.2 1.6 0.2]
 [4.8 3.1 1.6 0.2]
 [5.4 3.4 1.5 0.4]
 [5.2 4.1 1.5 0.1]
 [5.5 4.2 1.4 0.2]
 [4.9 3.1 1.5 0.2]
 [5.  3.2 1.2 0.2]
 [5.5 3.5 1.3 0.2]
 [4.9 3.6 1.4 0.1]
 [4.4 3.  1.3 0.2]
 [5.1 3.4 1.5 0.2]
 [5.  3.5 1.3 0.3]
 [4.5 2.3 1.3 0.3]
 [4.4 3.2 1.3 0.2]
 [5.  3.5 1.6 0.6]
 [5.1 3.8 1.9 0.4]
 [4.8 3.  1.4 0.3]
 [5.1 3.8 1.6 0.2]
 [4.6 3.2 1.4 0.2]
 [5.3 3.7 1.5 0.2]
 
#2 ç›®æ ‡å€¼
ç›®æ ‡å€¼
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]

#3 æè¿°
'.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher\'s paper. Note that it\'s the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n     Mathematical Statistics" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...'

#4 ç‰¹å¾å
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']

#5 æ ‡ç­¾å
array(['setosa', 'versicolor', 'virginica'], dtype='<U10')

```

**ğŸ‘†**  **ç”¨äºåˆ†ç±»å°æ•°æ®é›†**           **ğŸ‘‡** **ç”¨äºåˆ†ç±»å¤§æ•°æ®é›†**

```python
1.
sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train'
    1.1 subset:'train'æˆ–è€…'test','all',å¯é€‰ï¼Œé€‰æ‹©è¦åŠ è½½çš„æ•°æ®é›† --trainï¼šè®­ç»ƒé›†ï¼Œtestï¼šæµ‹è¯•é›†ï¼Œallï¼šä¸¤è€…å…¨éƒ¨ 
    1.2 datasets.clear_data_home(data_home=None)  --æ¸…é™¤ç›®å½•ä¸‹çš„æ•°æ®

#ä»£ç æ¡ˆä¾‹
from sklearn.datasets import fetch_20newsgroups
news = fetch_20newsgroups(subset='all')
```



**ç”¨äºå›å½’çš„æ•°æ®é›†ï¼š**

```python
1. sklearn.datasets.load_boston()  -- åŠ è½½å¹¶è¿”å›æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›† 
2. sklearn.datasets.load_diabetes()  -- åŠ è½½å’Œè¿”å›ç³–å°¿ç—…æ•°æ®é›†

# ä»£ç æ¡ˆä¾‹
from sklearn.datasets import load_boston 
lb = load_boston() 
print('ç‰¹å¾å€¼', lb.data) # ç›®æ ‡å€¼æ˜¯è¿ç»­çš„ å›å½’é—®é¢˜çš„ç‰¹æ€§ print('ç›®æ ‡å€¼', lb.target)
```



åˆ†ç±»é—®é¢˜é¢„æµ‹æ•°æ®å±äºå“ªä¸€ç±»åˆ«ã€‚ â€”â€” ç¦»æ•£
å›å½’é—®é¢˜æ ¹æ®æ•°æ®é¢„æµ‹ä¸€ä¸ªæ•°å€¼ã€‚ â€”â€” è¿ç»­



####  <font color='blue' face='æ¥·ä½“'>æ•°æ®é›†çš„åˆ’åˆ†</font>

**æ•°æ®é›†ä¼šä¸€èˆ¬åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è®­ç»ƒé›†ç”¨æ¥è®­ç»ƒï¼Œæ„å»ºæ¨¡å‹ï¼Œæµ‹è¯•é›†ç”¨æ¥è¯„ä¼°é¢„æµ‹ç»“æœã€‚**

**API:**

**sklearn.model_selection.train_test_split**

```python
from sklearn.model_selection import train_test_split
# x_train ,x_test ,y_train , y_test = train_test_split(x,y,test_size=0.*)
# x : æ•°æ®é›†çš„ç‰¹å¾å€¼
# y : æ•°æ®é›†çš„ç›®æ ‡å€¼
# test_size : æµ‹è¯•é›†çš„å¤§å°,ä¸€èˆ¬ä¸ºfloat
# radom_state : éšæœºæ•°ç§å­,ä¸åŒçš„ç§å­ä¼šé€ æˆä¸åŒçš„éšæœºé‡‡æ ·ç»“æœ.ç›¸åŒçš„ç§å­é‡‡æ ·ç»“æœç›¸åŒ
# return -- æ–¹æ³•è¿”å›4ä¸ªç»“æœ: è®­ç»ƒé›†ç‰¹å¾å€¼, æµ‹è¯•é›†ç‰¹å¾å€¼, è®­ç»ƒé›†ç›®æ ‡å€¼, æµ‹è¯•å€¼ç›®æ ‡å€¼(é»˜è®¤éšæœºå–å€¼)
```



```
x_train ,x_test , y_train , y_test  =
train_test_split(li.data,li.target,test_size=0.25) 
#ä¼ å…¥çš„å€¼ 1. ç‰¹å¾å€¼  2. ç›®æ ‡å€¼ 3. æµ‹è¯•é›†çš„å¤§å°  # æ¥æ”¶ä¸ºå›ºå®šå†™æ³•
x_train.shape  # è®­ç»ƒé›†å¤§å° 112æ¡   ä¸ºåŸæ•°æ®é›†çš„75%
li.data.shape  # åŸæ•°æ®é›†å¤§å°  150æ¡
```

![answer](./images/answer.png)



#### <font color='blue' face='æ¥·ä½“'>ç‰¹å¾æŠ½å–</font>

##### 	<font color='blue' face='æ¥·ä½“'>ç‰¹å¾å·¥ç¨‹</font>

â€‹	**ç‰¹å¾å·¥ç¨‹æ˜¯å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºæ›´å¥½åœ°ä»£è¡¨é¢„æµ‹æ¨¡å‹çš„æ½œåœ¨é—®é¢˜çš„ç‰¹å¾çš„è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†å¯¹æœªçŸ¥æ•°æ®çš„ é¢„æµ‹å‡†ç¡®æ€§**.

##### 	<font color='blue' face='æ¥·ä½“'>ç‰¹å¾æŠ½å–</font>

â€‹	**å¯¹æ–‡æœ¬ç­‰æ•°æ®è¿›è¡Œç‰¹å¾å€¼åŒ–, è®©è®¡ç®—æœºæ›´å¥½çš„ç†è§£æ•°æ®.**

#####		<font color='blue' face='æ¥·ä½“'>å¯¹å­—å…¸ç‰¹å¾æŠ½å–</font>

â€‹	**å¯¹å­—å…¸æ•°æ®è¿›è¡Œç‰¹å¾å€¼åŒ–. ä¸»è¦æ˜¯å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡ŒOne-hotç¼–ç .**

**API**

```python
sklearn.feature_extraction.DictVectorizer
1. DictVectorizer.fit_transform(X) 
    x:å­—å…¸æˆ–è€…åŒ…å«å­—å…¸çš„è¿­ä»£å™¨ 
    è¿”å›å€¼ï¼šè¿”å›sparseçŸ©é˜µï¼ˆç¨€ç–çŸ©é˜µï¼‰ 
2. DictVectorizer.inverse_transform(X) 
    x:arrayæ•°ç»„æˆ–è€…sparseçŸ©é˜µ
    è¿”å›å€¼ï¼šè½¬æ¢ä¹‹å‰æ•°æ®æ ¼å¼
3. DictVectorizer.get_feature_names()
    è¿”å›ç‰¹å¾åç§°
4. DictVectorizer.transform(x)
    æŒ‰ç…§åŸå…ˆçš„æ ‡å‡†è½¬æ¢ 
æµç¨‹ :
ã€€ã€€å®ä¾‹åŒ–ç±» DictVerctorizer
ã€€ã€€è°ƒç”¨ fit_transformæ–¹æ³•è¾“å…¥æ•°æ®å¹¶è½¬æ¢
```

**æ¡ˆä¾‹**

```python
# å¯¼å…¥æ¥å£
from sklearn.feature_extraction import DictVectorizer
# æ•°æ®
data = [{'name':'æ±‰å”åº—','Satisfaction':3.5},
        {'name':'èŠ±æºªåº—','Satisfaction':2.5},
        {'name':'æ¸©æ³‰åº—','Satisfaction':3}] 
# å®ä¾‹åŒ–
dv = DictVectorizer()
# è°ƒç”¨è½¬æ¢æ¥å£
res = dv.fit_transform(data)
print(res.toarray())   # è½¬æ¢æˆæ•°ç»„  çœ‹èµ·æ¥æ›´æ¸…æ™°
# ç»“æœæ³¨è§£ğŸ‘‡ : ç¬¬ä¸€ä¸ªç‰¹å¾å€¼ æ˜¯3.5 æ‰€ä»¥ç¬¬1ä¸ª æ˜¯1  ,äºŒä¸ªæ˜¯2.5â‰ 3.5  æ‰€ä»¥æ˜¯0  ,ç¬¬ä¸‰ä¸ªå€¼æ˜¯3 â‰ 3.5  ,æ‰€ä»¥æ˜¯0 ï¼›ç¬¬äºŒ,ç¬¬ä¸‰åˆ—ç±»ä¼¼ç¬¬ä¸€ç§
dv.get_feature_names()   # ç‰¹å¾åç§°
```

![anwser2](./images/anwser2.png)



#### <font color='blue' face='æ¥·ä½“'>å½’ä¸€åŒ–</font>

##### 	<font color='blue' face='æ¥·ä½“'>ç‰¹å¾é¢„å¤„ç†</font>

â€‹	**é€šè¿‡ç‰¹å®šçš„ç»Ÿè®¡æ–¹æ³•(æ•°å­¦æ–¹æ³•)å°†æ•°æ®è½¬æ¢æˆç®—æ³•è¦æ±‚çš„æ•°æ®**

**

```
ç±»åˆ«å‹æ•°æ® ï¼š one-hotç¼–ç 
æ—¶é—´ç±»å‹ : æ—¶é—´çš„åˆ‡åˆ†
```

**sklearnç‰¹å¾å¤„ç†API --->sklearn.preprocessing**

```python
ç±»ï¼š 
sklearn.prepocessing.MinMaxScaler
    MinMaxScaler(feature_range=(0,1)...) 
        æ¯ä¸ªç‰¹å¾ç¼©æ”¾åˆ°ç»™å®šèŒƒå›´ï¼ˆé»˜è®¤[0,1])
    MinMaxScalar.fit_transform(x)
         x:numpy arrayæ ¼å¼çš„æ•°æ®ï¼ˆn_samples,n_features) 
        è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array     
æ­¥éª¤
    å®ä¾‹åŒ– MinMaxScalar
    é€šè¿‡ fit_transform è½¬æ¢
```



```python
# ç‰¹å¾é¢„å¤„ç†--æ•°æ®
data = np.array([[5000,2,10,40],[6000,3,15,45],[50000,5,15,40]])

#--------------------#
array([[ 5000,     2,    10,    40],
       [ 6000,     3,    15,    45],
       [50000,     5,    15,    40]])
```

æœ‰æ—¶å€™ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°æŸäº›ç‰¹å¾æ¯”å…¶ä»–ç‰¹å¾æ‹¥æœ‰é«˜å¾—å¤šçš„è·¨åº¦å€¼ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå°†ä¸€ä¸ªäººçš„ **æ”¶å…¥**å’Œä»–çš„ **å¹´é¾„**è¿›è¡Œæ¯”è¾ƒï¼Œé€šè¿‡ç¼©æ”¾å¯ä»¥é¿å…æŸäº›ç‰¹å¾æ¯”å…¶ä»–ç‰¹å¾è·å¾—å¤§å°éå¸¸æ‚¬æ®Šçš„æƒé‡å€¼ã€‚å³ä½¿å¾—æŸä¸€ä¸ªç‰¹å¾ ä¸ä¼šå¯¹ç»“æœé€ æˆè¿‡å¤§çš„å½±å“.

#####		<font color='blue' face='æ¥·ä½“'>æ•°å€¼ç±»å‹æ ‡å‡†æ•°æ®ç¼©æ”¾</font>

â€‹	**å½’ä¸€åŒ–**

â€‹	**`ç‰¹ç‚¹ï¼šé€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢å§æ•°æ®æ˜ å°„åˆ°ï¼ˆé»˜è®¤ä¸º[0,1]ï¼‰ä¹‹é—´`**

```python
# å¯¼å…¥æ¥å£
from sklearn.preprocessing import MinMaxScaler
# å®ä¾‹åŒ–
mm = MinMaxScaler()  # é»˜è®¤0-1ä¹‹é—´
# å¯¹æ•°æ®è¿›è¡Œè½¬æ¢-å½’ä¸€åŒ–
res = mm.fit_transform(data)   
res
```

![answer3](./images/answer3.png)

è¿˜å¯ä»¥æŒ‡å®šèŒƒå›´ç¼©å‡

```
mm = MinMaxScaler(feature_range=(0,3))
res = mm.fit_transform(data)
res
```

![answer4](./images/anwser4.png)

**ç¼ºç‚¹:** **<font size='2'>åœ¨ç‰¹å®šçš„åœºæ™¯ä¸‹å¤§å€¼å°å€¼æ˜¯å˜åŒ–çš„ï¼Œå¦å¤–ï¼Œå¤§å€¼ä¸å°å€¼éå¸¸å®¹æ˜“å—å¼‚å¸¸ç‚¹çš„å½±å“ï¼Œæ‰€ä»¥è¿™ç§æ–¹ æ³•çš„é²æ£’æ€§(ç¨³å®šæ€§)è¾ƒå·®ï¼Œåªé€‚åˆä¼ ç»Ÿç²¾ç¡®å°æ•°æ®åœºæ™¯ã€‚æ‰€ä»¥ä¸€èˆ¬ä¸ä¼šä½¿ç”¨ï¼Œå¹¿æ³›ä½¿ç”¨çš„æ˜¯æ ‡å‡†åŒ–ã€‚</font>**



####  <font color='blue' face='æ¥·ä½“'>æ ‡å‡†åŒ–ä¸ç¼ºå¤±å€¼çš„å¤„ç†Â </font>

â€‹	**æ ‡å‡†åŒ–**

â€‹	ç‰¹ç‚¹ï¼šé€šè¿‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œå˜æ¢æŠŠæ•°æ®å˜æ¢åˆ°å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„èŒƒå›´å†…ã€‚

![å…¬å¼](./images/å…¬å¼.png)

<font color='green' size='2'>å¯¹äºå½’ä¸€åŒ–æ¥è¯´ï¼šå¦‚æœå‡ºç°å¼‚å¸¸ç‚¹ï¼Œå½±å“äº†å¤§å€¼å’Œå°å€¼ï¼Œé‚£ä¹ˆç»“æœæ˜¾ç„¶ä¼šå‘ç”Ÿæ”¹å˜</font>

<font color='green' size='2'>å¯¹äºæ ‡å‡†åŒ–æ¥è¯´ï¼šå¦‚æœå‡ºç°å¼‚å¸¸ç‚¹ï¼Œç”±äºå…·æœ‰ä¸€å®šæ•°æ®é‡ï¼Œå°‘é‡çš„å¼‚å¸¸ç‚¹å¯¹äºå¹³å‡å€¼çš„å½±å“å¹¶ä¸å¤§ï¼Œä» è€Œæ–¹å·®æ”¹å˜è¾ƒå°ã€‚åœ¨å·²æœ‰æ ·æœ¬è¶³å¤Ÿå¤šçš„æƒ…å†µä¸‹æ¯”è¾ƒç¨³å®šï¼Œé€‚åˆç°ä»£å˜ˆæ‚çš„å¤§æ•°æ®åœºæ™¯ã€‚</font>



â€‹	**æ ‡å‡†åŒ–APIï¼š**

```python
API : sklearn.preprocessing.StandarScaler
StandardScaler() 
    å¤„ç†ä¹‹åæ¯åˆ—æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨å‡å€¼ä¸º0æ ‡å‡†å·®ä¸º1é™„è¿‘
    StandarScaler.fit_transform(x) 
        x:numpy arrayæ ¼å¼çš„æ•°æ®(n_samples,n_features) 
        è¿”å›å€¼ï¼šè½¬æ¢åçš„å½¢çŠ¶ç›¸åŒçš„array 
    StandarScaler.mean_ 
        åŸå§‹æ•°æ®ä¸­æ¯åˆ—çš„å¹³å‡å€¼ä¸­ä½æ•° 
    StandarScaler.std_ 
        åŸå§‹ä¸»å¥æ¯åˆ—ç‰¹å¾çš„æ–¹å·® 
æ­¥éª¤ 
    å®ä¾‹åŒ– StandarScaler 
    é€šè¿‡ fit_transform è½¬æ¢
```



**æ¡ˆä¾‹**

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
ss = StandardScaler() #å®ä¾‹åŒ–
data = np.array([[5000,2,10,40],[6000,3,15,45],[50000,5,15,40]]) #æ•°æ®
res = ss.fit_transform(data) 
print(res)
```

![answer5](./images/answer5.png)





**ç¼ºå¤±å€¼çš„å¤„ç†æ–¹æ³• : **
ã€€ã€€<font color='red'>åˆ é™¤ </font>**: å¦‚æœæ¯åˆ—æˆ–è€…è¡Œæ•°æ®è¾¾åˆ°ä¸€å®šæ¯”ä¾‹, å»ºè®®æ”¾å¼ƒæ•´è¡Œæˆ–æ•´åˆ— **

ã€€ã€€<font color='red'>æ’è¡¥</font> **: å¯ä»¥é€šè¿‡ç¼ºå¤±å€¼æ¯è¡Œæˆ–æ¯åˆ—çš„å¹³å‡å€¼/ä¸­ä½æ•°æ¥å¡«å……**



**ç¼ºå¤±å€¼å¤„ç†API:**

```python
sklearnç¼ºå¤±å€¼æ¥å£ : sklearn.impute.SimpleImputer
   SimpleImputer(missing_values='NaN',strategy='mean',axis=0) 
       å®Œæˆç¼ºå¤±å€¼æ’è¡¥
   SimpleImputer.fit_transform(x) 
        x:numpy arrayæ ¼å¼çš„æ•°æ® 
        è¿”å›å€¼ï¼šè½¬æ¢åå½¢çŠ¶ç›¸åŒä¹Ÿå³æ˜¯çš„array 
æ­¥éª¤
    åˆå§‹åŒ–SimpleImputer,æŒ‡å®šç¼ºå¤±å€¼ï¼ŒæŒ‡å®šå¡«è¡¥ç­–ç•¥ï¼ŒæŒ‡å®šè¡Œæˆ–åˆ—ã€‚ æ³¨ï¼šç¼ºå¤±å€¼ä¹Ÿå¯ä»¥æ˜¯åˆ«çš„æŒ‡å®šè¦æ›¿æ¢çš„å€¼ 
    è°ƒç”¨ã€€ï¬t_transform
```



**æ¡ˆä¾‹**

```python
import numpy as np
from sklearn.impute import SimpleImputer
data = np.array([[1,2],[np.NaN,3],[7,6]])
si = SimpleImputer()
# si = SimpleImputer(missing_values=6,strategy='most_frequent')  
# missing_values : æŒ‡å®šå€¼å¡«å……,é»˜è®¤ä¸ºNaN. 
# strategy : å¡«å……æ•°å€¼è®¡ç®—æ–¹æ³•  ['mean', 'median', 'most_frequent', 'constant']
res = si.fit_transform(data)
print(res)
```

![anwser5](./images/anwser5png.png)

**ğŸ‘‰ åˆ—avgï¼Œä¸åŒ…å«NaNï¼Œä¸è®¡ç®—NaNè¡Œ **



#### <font color='blue' face='æ¥·ä½“'>æ•°æ®é™ç»´</font>

â€‹	**æ•°æ®é™ç»´:**`è¿™é‡Œçš„ç»´åº¦æŒ‡å®šæ˜¯ç‰¹å¾æ•°é‡ï¼Œè¿™é‡Œçš„é™ç»´æ˜¯æŒ‡å‡å°‘ç‰¹å¾çš„æ•°é‡ã€‚`

â€‹	**æ•°æ®é™ç»´æœ‰2ç§,åˆ†åˆ«æ˜¯ç‰¹å¾é€‰æ‹© å’Œ ä¸»æˆåˆ†åˆ†æ.**

##### ç‰¹å¾é€‰æ‹©

â€‹	**ç‰¹å¾é€‰æ‹©çš„åŸå› ï¼š**

â€‹			**å†—ä½™ : éƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³åº¦é«˜ï¼Œå®¹æ˜“æ¶ˆåŒ–è®¡ç®—æ€§èƒ½**.

ã€€ã€€ã€€**å™ªå£° : éƒ¨åˆ†ç‰¹å¾å¯¹é¢„æµ‹ç»“æœæœ‰å½±å“.**

ã€€**æ¦‚å¿µ :**

ã€€ã€€**ç‰¹å¾é€‰æ‹©å°±æ˜¯æŒ‡ å•çº¯çš„ä»æå–åˆ°çš„æ‰€æœ‰ç‰¹å¾é€‰æ‹©éƒ¨åˆ†ç‰¹å¾ä½œä¸ºè®­ç»ƒé›†ç‰¹å¾, ç‰¹å¾åœ¨é€‰æ‹©å‰å’Œé€‰æ‹©åå¯ä»¥æ”¹å˜å€¼,ä¹Ÿå¯ä»¥ä¸æ”¹å˜å€¼.ä½†æ˜¯é€‰æ‹©åçš„ç‰¹å¾ç»´æ•°è‚¯å®šæ¯”é€‰æ‹©å‰å°,æ¯•ç«Ÿæˆ‘ä»¬åªé€‰æ‹©äº†å…¶ä¸­çš„ä¸€éƒ¨åˆ†.**

ã€€**ä¸»è¦æ–¹æ³•:**

ã€€ã€€ã€€ã€€**Filter(è¿‡æ»¤å¼): VarianceThreshold**

ã€€ã€€ã€€ã€€**Embedded(åµŒå…¥å¼): æ­£åˆ™åŒ– , å†³ç­–æ ‘**

**ç‰¹å¾é€‰æ‹©API (é™ç»´):**

```python
ç±»ï¼šsklearn.feature_selection.VarianceThreshold 

VarianceThreshold(threshold=0.0) 
    åˆ é™¤æ‰€æœ‰ä½æ–¹å·®ç‰¹å¾ 
Variance.ï¬t_transform(x) 
    x:numpy arrayæ ¼å¼çš„æ•°æ® 
    è¿”å›å€¼ï¼šåˆ é™¤æ–¹å·®ä½äºthresholdçš„ç‰¹å¾ä¹‹åçš„æ•°æ®é›† 
    é»˜è®¤å€¼æ˜¯ä¿ç•™æ‰€æœ‰éé›¶æ–¹å·®ç‰¹å¾ï¼Œå³åˆ é™¤æ‰€æœ‰æ ·æœ¬ä¸­å…·æœ‰ç›¸åŒå€¼çš„ç‰¹å¾ 
æµç¨‹ 
    åˆå§‹åŒ–VarianceThreshold,æŒ‡å®šé˜ˆå€¼æ–¹å·®
    è°ƒç”¨ ï¬t_transform
```

**æ¡ˆä¾‹**

```python
# æ¡ˆä¾‹æ¼”ç¤º
import numpy as np
from sklearn.feature_selection import VarianceThreshold
data = np.array([[0,2,0,3],[0,1,4,3],[0,1,2,3]])
print('é™ç»´å‰:',data)
vt = VarianceThreshold(threshold=0.0)  #è¿‡æ»¤æ–¹å·®ä¸º0 , æ•°æ®ä¸å˜
res = vt.fit_transform(data)   # å°‘éƒ¨åˆ†æ•°æ®
print('é™ç»´å:',res)
```

![answer5](./images/answer6.png)

##### <font color='blue' face='æ¥·ä½“'>ä¸»æˆåˆ†åˆ†æï¼š</font>

ã€€	æœ¬è´¨ : PCAæ˜¯ä¸€ç§åˆ†æ,ç®€åŒ–æ•°æ®é›†çš„æŠ€æœ¯.

ã€€ã€€ç›®çš„ : æ˜¯æ•°æ®ç»´åº¦å‹ç¼©,å°½å¯èƒ½é™ä½åŸæ•°æ®çš„ç»´æ•°(å¤æ‚åº¦),å°½å¯èƒ½çš„å‡å°‘æŸå¤±ä¿¡æ¯

ã€€ã€€ä½œç”¨ : å¯ä»¥æ¶ˆå‡å›å½’åˆ†ææˆ–è€…èšç±»åˆ†æä¸­ç‰¹å¾çš„æ•°é‡

ã€€ã€€ä½¿ç”¨åœºæ™¯ : ç‰¹å¾æ•°é‡è¾¾åˆ°ä¸Šç™¾çš„æ—¶å€™,è€ƒè™‘æ•°æ®çš„ç®€åŒ–.



**ä¸»æˆåˆ†åˆ†æAPI(é™ç»´) :**

```python
ç±» sklearn.decompositon.PCAPCA(n_componets=None) 
    å°†æ•°æ®åˆ†è§£ä¸ºè¾ƒä½ç»´æ•°æ® 
    PCA.ï¬t_transform(x) 
        x:numpy arrayæ ¼å¼ 
        è¿”å›å€¼ï¼šè½¬æ¢åé™ä½ç»´åº¦çš„array 
    n_componetså‚æ•°ï¼š 
        å°æ•°ï¼šè¡¨ç¤ºå°†ä¿¡æ¯ä¿å­˜åˆ°åŸä¿¡æ¯çš„ç™¾åˆ†æ¯”ï¼Œä¾‹å¦‚0.95è¡¨ç¤ºé™ç»´åä¿¡æ¯é‡æ˜¯åŸæ¥çš„95%ã€‚ä¸€èˆ¬åˆ¶å®šåˆ°0.9-0.95 
        æ•´æ•°ï¼šè¾ƒå°‘åˆ°çš„ç‰¹å¾æ•°é‡ï¼Œä¸€èˆ¬ä¸ä½¿ç”¨ 
    æµç¨‹
        å®ä¾‹åŒ– PCA 
        è°ƒç”¨ ï¬t_transform
```

**æ¡ˆä¾‹**

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
data = np.array([[2,8,4,5],[6,3,0,8],[5,4,9,1]])
print('ä¸»æˆåˆ†åˆ†æé™ç»´å‰:\n',data)
res = pca.fit_transform(data)
print('ä¸»æˆåˆ†åˆ†æé™ç»´å:\n',res)
# ç»´åº¦ä¸‹é™,ä¸æŸå¤±ä¿¡æ¯
```

![answer7](./images/answer7.png)

##### APIæ€»ç»“ :

ã€€ã€€**1. fit_transform() : è¾“å…¥æ•°æ®ç›´æ¥è½¬æ¢**

ã€€ã€€**2. fit() : è¾“å…¥æ•°æ®,è®¡ç®—ä¸€äº›ä¸­é—´å€¼,ä½†æ˜¯ä¸èƒ½è½¬æ¢**

ã€€ã€€**3. transform() : è¿›è¡Œæ•°æ®è½¬æ¢**

ã€€ã€€**4. fit_transform() = fit() + transform()**
